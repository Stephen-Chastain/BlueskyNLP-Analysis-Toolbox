{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d1b5ad8",
   "metadata": {},
   "source": [
    "Step 1: import Bluesky Firehose Summer 2025 post data from Snowflake via a cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57b2391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import re\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from time import sleep\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "### GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS \n",
    "### GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS \n",
    "### GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS \n",
    "\n",
    "# this basically means \"smoke em if you got em\" where the \"em\" is NVIDIA GPU\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "SF_USR = os.getenv('SF_USR')\n",
    "SF_ID  = os.getenv('SF_ID')\n",
    "SF_WH  = os.getenv('SF_WH')\n",
    "SF_DB  = os.getenv('SF_DB')\n",
    "SF_SC  = os.getenv('SF_SC')\n",
    "SF_RL  = os.getenv('SF_RL')\n",
    "\n",
    "# connect to database and init a cursor for querying\n",
    "xct_params = {\n",
    "    \"user\":                 SF_USR\n",
    "   ,\"account\":              SF_ID\n",
    "   ,\"warehouse\":            SF_WH\n",
    "   ,\"database\":             SF_DB\n",
    "   ,\"schema\":               SF_SC\n",
    "   ,\"role\":                 SF_RL\n",
    "   ,\"private_key_file\":     os.getenv('PRIVATE_KEY_PATH')\n",
    "   ,\"private_key_file_pwd\": os.getenv('PRIVATE_KEY_PASSPHRASE')\n",
    "   ,\"authenticator\":        os.getenv('SF_AUTH')\n",
    "}\n",
    "\n",
    "SF_XCT = snowflake.connector.connect(**xct_params) #connection object\n",
    "CSR = SF_XCT.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d94af5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import ccf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4486d4",
   "metadata": {},
   "source": [
    "Step 2: Query the database. \n",
    "\n",
    "Our sampling methodology is an interrupted census (every post for a 5 minute interval, every 3 hours throughout the day), not a random sample, and not a continuous sample.\n",
    "\n",
    "Therefore for timeseries analysis, we GROUP BY the day field to alow for a continuous timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b301f49",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"select\n",
    "  keyword,\n",
    "  topic,\n",
    "  category,\n",
    "  DAY_POST_CREATED_AT,\n",
    "  max(MONTH_POST_CREATED_AT) as month_post_created_at,\n",
    "  max(YEAR_POST_CREATED_AT) as year_post_created_at,\n",
    "  COUNT(author_thread_mentions) daily_author_thread_mentions,\n",
    "  SUM(total_posts_with_keyword) as daily_total_posts_keyword,\n",
    "  SUM(positive_mentions) AS daily_positive_mentions,\n",
    "  SUM(negative_mentions) AS daily_negative_mentions,\n",
    "  SUM(neutral_mentions) AS daily_neutral_mentions,\n",
    "  AVG(pct_positive) as daily_pct_positive,\n",
    " AVG(pct_negative) as daily_pct_negative,\n",
    " AVG(pct_neutral) as daily_pct_neutral,\n",
    "from {SF_DB}.{SF_SC}.timeseries_nlp\n",
    "group by \n",
    "keyword\n",
    ",topic\n",
    ",category\n",
    ",day_post_created_at\n",
    "order by month_post_created_at,day_post_created_at asc;\"\"\" \n",
    "CSR.execute(query)\n",
    "df = CSR.fetch_pandas_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b742fca4",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KEYWORD                         124\n",
       "TOPIC                           124\n",
       "CATEGORY                        124\n",
       "DAY_POST_CREATED_AT             124\n",
       "MONTH_POST_CREATED_AT           124\n",
       "YEAR_POST_CREATED_AT            124\n",
       "DAILY_AUTHOR_THREAD_MENTIONS    124\n",
       "DAILY_TOTAL_POSTS_KEYWORD       124\n",
       "DAILY_POSITIVE_MENTIONS         124\n",
       "DAILY_NEGATIVE_MENTIONS         124\n",
       "DAILY_NEUTRAL_MENTIONS          124\n",
       "DAILY_PCT_POSITIVE              124\n",
       "DAILY_PCT_NEGATIVE              124\n",
       "DAILY_PCT_NEUTRAL               124\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421a01d",
   "metadata": {},
   "source": [
    "Step 3: Format Timestamp Column\n",
    "\n",
    "This step creates a pandas-formatted timestamp that we can use for all the remaining tests in this notebook.\n",
    "\n",
    "Timeseries data columns (hour, day, month, year) are stored as separate numeric columns in the dataset to allow for other groupins and analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "887eea68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  KEYWORD             TOPIC        CATEGORY  DAY_POST_CREATED_AT  \\\n",
      "0   china  global conflicts  issue specific                   17   \n",
      "1   trump       US domestic  issue specific                   17   \n",
      "2   biden       US domestic  issue specific                   17   \n",
      "3   putin  global conflicts  issue specific                   17   \n",
      "4   putin  global conflicts  issue specific                   18   \n",
      "\n",
      "   MONTH_POST_CREATED_AT  YEAR_POST_CREATED_AT  DAILY_AUTHOR_THREAD_MENTIONS  \\\n",
      "0                      7                  2025                            17   \n",
      "1                      7                  2025                            22   \n",
      "2                      7                  2025                            16   \n",
      "3                      7                  2025                            17   \n",
      "4                      7                  2025                            17   \n",
      "\n",
      "  DAILY_TOTAL_POSTS_KEYWORD  DAILY_POSITIVE_MENTIONS  DAILY_NEGATIVE_MENTIONS  \\\n",
      "0                       167                       24                       69   \n",
      "1                      3999                      109                     3047   \n",
      "2                       266                        8                      209   \n",
      "3                       268                        5                      195   \n",
      "4                       206                       14                      127   \n",
      "\n",
      "   DAILY_NEUTRAL_MENTIONS DAILY_PCT_POSITIVE DAILY_PCT_NEGATIVE  \\\n",
      "0                      74    13.111120117647    38.244405235294   \n",
      "1                     804     6.538820227273    72.664347590909   \n",
      "2                      42     3.756127500000    79.651517250000   \n",
      "3                      64     1.346630647059    72.586512352941   \n",
      "4                      64     5.432661941176    60.127697352941   \n",
      "\n",
      "  DAILY_PCT_NEUTRAL  TIMESTAMP  \n",
      "0   49.379768764706 2025-07-17  \n",
      "1   20.973297727273 2025-07-17  \n",
      "2   16.592355375000 2025-07-17  \n",
      "3   26.525451235294 2025-07-17  \n",
      "4   34.439640647059 2025-07-18  \n"
     ]
    }
   ],
   "source": [
    "df['TIMESTAMP'] = pd.to_datetime(dict(year=df['YEAR_POST_CREATED_AT'], month=df['MONTH_POST_CREATED_AT'], day=df['DAY_POST_CREATED_AT']))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75edb0a8",
   "metadata": {},
   "source": [
    "Step 4: Determine which keyword sentiments show a trend vs a stationary. \n",
    "\n",
    "The Augmented Dickey-Fuller Test identifies whether a quantity has a trend within a timeseries.\n",
    "This test cannot determine the direction of the trend on its own. Those keywords showing a trend will be analyzed with decomposition analysis and autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d4838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adf_stats(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Performs the Augmented Dickey-Fuller test and returns statistics\n",
    "    as dictionary with p-value and stationarity status.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = adfuller(series, autolag='AIC')\n",
    "        p_value = result[1]\n",
    "        is_stationary = p_value <= 0.05\n",
    "        return {'adf_p_value': p_value, 'is_stationary': is_stationary}\n",
    "    except Exception as e:\n",
    "        print(f\"Error during ADF test: {e}\")\n",
    "        return {'adf_p_value': np.nan, 'is_stationary': np.nan}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52507ad",
   "metadata": {},
   "source": [
    "Step 5: Perform timeseries tests: decomposition runs conditionally on keywords with an identified trend. \n",
    "\n",
    "If there is a trend, decomposition  separates out seasonality and residual. This is only performed on keywords where a trend is identified.\n",
    "\n",
    "We compare the residuals from both additive and multiplicative models. The one with a more random distribution of residuals is the better match to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62dd99af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decomposition_stats(series: pd.Series, model: str, series_name: str):\n",
    "    \"\"\"\n",
    "    Performs seasonal decomposition on a time series using a specified model\n",
    "    and returns statistics as a dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Default is 7 day period for weekly seasonality component.\n",
    "        decomposition = seasonal_decompose(series, model=model, period=7)\n",
    "\n",
    "        # Populate dictionary output with decomp statistics: trend, seasonal, residual\n",
    "        trend_stats = {\n",
    "            'trend_mean': decomposition.trend.mean(),\n",
    "            'trend_std': decomposition.trend.std()\n",
    "        }\n",
    "        \n",
    "        seasonal_stats = {\n",
    "            'seasonal_range': decomposition.seasonal.max() - decomposition.seasonal.min()\n",
    "        }\n",
    "\n",
    "        residual_stats = {\n",
    "            'residual_std': decomposition.resid.std()\n",
    "        }\n",
    "\n",
    "        # Combine all stats into a single dictionary\n",
    "        all_decomp_stats = {**trend_stats, **seasonal_stats, **residual_stats}\n",
    "        \n",
    "        # Add a prefix to each key to indicate the decomposition model used\n",
    "        prefixed_decomp_stats = {f\"{model}_{series_name}_{key}\": value for key, value in all_decomp_stats.items()}\n",
    "        return prefixed_decomp_stats\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during {model} decomposition for {series_name} series: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00689a65",
   "metadata": {},
   "source": [
    "Pearson correlation test for one trend vs another, assuming no trend in the data (i.e. from the result of ADF test above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a513e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pearson_correlation_stats(series_a: pd.Series, series_b: pd.Series):\n",
    "    \"\"\"\n",
    "    Calculates the Pearson Correlation Coefficient (zero-lag correlation) between two series.\n",
    "    Assumes that the input series are already made stationary (e.g., by differencing).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Align series by index and drop any NaN values\n",
    "        combined = pd.concat([series_a.rename('a'), series_b.rename('b')], axis=1).dropna()\n",
    "        \n",
    "        if len(combined) < 2: \n",
    "            return {'pearson_r': np.nan, 'abs_pearson_r': np.nan}\n",
    "\n",
    "        # Calculate the zero-lag Pearson correlation coefficient\n",
    "        pearson_r = combined['a'].corr(combined['b'], method='pearson')\n",
    "\n",
    "        return {\n",
    "            'pearson_r': pearson_r,\n",
    "            'abs_pearson_r': abs(pearson_r)\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Pearson Correlation test: {e}\")\n",
    "        return {'pearson_r': np.nan, 'abs_pearson_r': np.nan}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6366e0e",
   "metadata": {},
   "source": [
    "Step 6: Run the pipeline script that stitches the tests together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b29a7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs the full timeseries analysis pipeline\n",
    "def timeseries_analysis_pipeline(df: pd.DataFrame, keyword_column: str = 'KEYWORD', timestamp_column: str = 'TIMESTAMP'):\n",
    "    \"\"\"\n",
    "    Runs a statistical analysis pipeline on the timeseries DataFrame ('df' by default here).\n",
    "    It expects a DataFrame with a 'TIMESTAMP' column for daily time series index\n",
    "    and the 'KEYWORD' column for grouping the data. Keywords are determined by table object in Snowflake (steets schema) prior to analysis.\n",
    "    It will perform ADF tests\n",
    "    and conditional, seasonal decomposition (if trend identified) and pearson's correlation test on the specified data columns.\n",
    "    Pearson's runs on \n",
    "    \"\"\"\n",
    "    # Define the metric columns for analysis-- both percentages and thread counts.\n",
    "    time_series_columns = [\n",
    "        'DAILY_PCT_POSITIVE',\n",
    "        'DAILY_PCT_NEGATIVE',\n",
    "        'DAILY_POSITIVE_MENTIONS',\n",
    "        'DAILY_NEGATIVE_MENTIONS'\n",
    "    ]\n",
    "#########set up variables and keyword mentions filter    \n",
    "    # Define the two model types for decomp\n",
    "    decomposition_models = ['additive', 'multiplicative']\n",
    "    #Store all the results\n",
    "    pipeline_results = []   \n",
    "    \n",
    "    #Specifying minimum number of mentions to run the correlation analysis (avoids spurrious correlations with low data). \n",
    "    #This number is just arbitrary.\n",
    "    minimum_mentions = 100 \n",
    "    corr_series_a = 'DAILY_POSITIVE_MENTIONS' \n",
    "    corr_series_b = 'DAILY_NEGATIVE_MENTIONS'\n",
    "    \n",
    "    # Group the DataFrame by keyword for a row-by-row analysis\n",
    "    grouped = df.groupby('KEYWORD')\n",
    "    \n",
    "    for KEYWORD, group_df in grouped:\n",
    "        # Sort by TIMESTAMP to ensure the time series is in order\n",
    "        group_df = group_df.set_index('TIMESTAMP').sort_index()\n",
    "\n",
    "        # Create a base dictionary for this keyword's results\n",
    "        row_stats = {'KEYWORD': KEYWORD}\n",
    "\n",
    "        # Dictionaries to store series and stationarity status for correlation\n",
    "        series_for_corr = {}\n",
    "        stationary_status = {}\n",
    "\n",
    "        #filter to minimum keyword mentions\n",
    "        run_correlation = group_df[corr_series_a].sum() >= minimum_mentions\n",
    "#########  APPLY STATISTICAL TESTS\n",
    "        # Loop through each specified time series column\n",
    "        for col in time_series_columns:\n",
    "            series = group_df[col]\n",
    "            \n",
    "            #Run the ADF test\n",
    "            adf_stats = get_adf_stats(series)\n",
    "            stationary_status[col] = adf_stats['is_stationary']\n",
    "\n",
    "            #if the series is stationary we use the data as-is. if not, we run series.diff() to difference it\n",
    "            if not adf_stats ['is_stationary']:\n",
    "                series_for_corr[col] = series.diff().dropna()\n",
    "            else:\n",
    "                series_for_corr[col] = series.dropna()\n",
    "\n",
    "\n",
    "            # Add ADF results to the dictionary with a descriptive column header\n",
    "            for key, value in adf_stats.items():\n",
    "                row_stats[f\"{col}_{key}\"] = value\n",
    "\n",
    "############# Conditionally run the decomposition tests\n",
    "            if not adf_stats['is_stationary']:\n",
    "                print(f\"Running decomposition for '{KEYWORD}' on column '{col}'...\")\n",
    "                for model in decomposition_models:\n",
    "                    stats = get_decomposition_stats(series, model, col)\n",
    "                    row_stats.update(stats)\n",
    "            else:\n",
    "                print(f\"Skipping decomposition for '{KEYWORD}' on column '{col}' (stationary).\")\n",
    "                # Create placeholder columns with NA values\n",
    "                for model in decomposition_models:\n",
    "                    for stat_key in ['trend_mean', 'trend_std', 'seasonal_range', 'residual_std']:\n",
    "                        row_stats[f\"{model}_{col}_{stat_key}\"] = np.nan\n",
    "            \n",
    "\n",
    "############# Next: run Pearson test for correlations on non-trending keywords\n",
    "        if run_correlation and corr_series_a in series_for_corr and corr_series_b in series_for_corr:\n",
    "            print(f\"Running Pearson correlation for '{KEYWORD}' on {'differenced' if not stationary_status[corr_series_a] else 'raw'} data...\")\n",
    "            \n",
    "            pos_series_adj = series_for_corr[corr_series_a]\n",
    "            neg_series_adj = series_for_corr[corr_series_b]\n",
    "\n",
    "            correlation_stats = get_pearson_correlation_stats(pos_series_adj, neg_series_adj)\n",
    "            \n",
    "            # Update dictionary keys\n",
    "            for key, value in correlation_stats.items():\n",
    "                row_stats[f\"Pearson_{corr_series_a}_vs_{corr_series_b}_{key}\"] = value\n",
    "        else:\n",
    "            print(f\"Skipping Pearson correlation for '{KEYWORD}' (mentions < {minimum_mentions}).\")\n",
    "            # Create placeholder columns with NA values\n",
    "            row_stats[f\"Pearson_{corr_series_a}_vs_{corr_series_b}_pearson_r\"] = np.nan\n",
    "            row_stats[f\"Pearson_{corr_series_a}_vs_{corr_series_b}_abs_pearson_r\"] = np.nan\n",
    "\n",
    "        pipeline_results.append(row_stats)\n",
    "    \n",
    "    # Convert the list of dictionaries to the final summary DataFrame\n",
    "    final_df = pd.DataFrame(pipeline_results)\n",
    "    \n",
    "    print(\"--- Final Summary Table Results ---\")\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09828967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping decomposition for 'biden' on column 'DAILY_PCT_POSITIVE' (stationary).\n",
      "Skipping decomposition for 'biden' on column 'DAILY_PCT_NEGATIVE' (stationary).\n",
      "Skipping decomposition for 'biden' on column 'DAILY_POSITIVE_MENTIONS' (stationary).\n",
      "Skipping decomposition for 'biden' on column 'DAILY_NEGATIVE_MENTIONS' (stationary).\n",
      "Running Pearson correlation for 'biden' on raw data...\n",
      "Running decomposition for 'china' on column 'DAILY_PCT_POSITIVE'...\n",
      "Skipping decomposition for 'china' on column 'DAILY_PCT_NEGATIVE' (stationary).\n",
      "Skipping decomposition for 'china' on column 'DAILY_POSITIVE_MENTIONS' (stationary).\n",
      "Skipping decomposition for 'china' on column 'DAILY_NEGATIVE_MENTIONS' (stationary).\n",
      "Running Pearson correlation for 'china' on raw data...\n",
      "Skipping decomposition for 'putin' on column 'DAILY_PCT_POSITIVE' (stationary).\n",
      "Skipping decomposition for 'putin' on column 'DAILY_PCT_NEGATIVE' (stationary).\n",
      "Running decomposition for 'putin' on column 'DAILY_POSITIVE_MENTIONS'...\n",
      "Running decomposition for 'putin' on column 'DAILY_NEGATIVE_MENTIONS'...\n",
      "Running Pearson correlation for 'putin' on differenced data...\n",
      "Skipping decomposition for 'trump' on column 'DAILY_PCT_POSITIVE' (stationary).\n",
      "Skipping decomposition for 'trump' on column 'DAILY_PCT_NEGATIVE' (stationary).\n",
      "Running decomposition for 'trump' on column 'DAILY_POSITIVE_MENTIONS'...\n",
      "Running decomposition for 'trump' on column 'DAILY_NEGATIVE_MENTIONS'...\n",
      "Running Pearson correlation for 'trump' on differenced data...\n",
      "--- Final Summary Table Results ---\n"
     ]
    }
   ],
   "source": [
    "final_results_df = timeseries_analysis_pipeline(df, keyword_column='KEYWORD', timestamp_column='TIMESTAMP')\n",
    "\n",
    "#testing results\n",
    "final_results_df.to_csv('pipeline_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
