{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d1b5ad8",
   "metadata": {},
   "source": [
    "Step 1: import Bluesky Firehose Summer 2025 post data from Snowflake via a cursor (PFC code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57b2391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import re\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from time import sleep\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "### GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS \n",
    "### GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS \n",
    "### GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS | GLOBAL VARS \n",
    "\n",
    "# this basically means \"smoke em if you got em\" where the \"em\" is NVIDIA GPU\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "SF_USR = os.getenv('SF_USR')\n",
    "SF_ID  = os.getenv('SF_ID')\n",
    "SF_WH  = os.getenv('SF_WH')\n",
    "SF_DB  = os.getenv('SF_DB')\n",
    "SF_SC  = os.getenv('SF_SC')\n",
    "SF_RL  = os.getenv('SF_RL')\n",
    "\n",
    "# connect to database and init a cursor for querying\n",
    "xct_params = {\n",
    "    \"user\":                 SF_USR\n",
    "   ,\"account\":              SF_ID\n",
    "   ,\"warehouse\":            SF_WH\n",
    "   ,\"database\":             SF_DB\n",
    "   ,\"schema\":               SF_SC\n",
    "   ,\"role\":                 SF_RL\n",
    "   ,\"private_key_file\":     os.getenv('PRIVATE_KEY_PATH')\n",
    "   ,\"private_key_file_pwd\": os.getenv('PRIVATE_KEY_PASSPHRASE')\n",
    "   ,\"authenticator\":        os.getenv('SF_AUTH')\n",
    "}\n",
    "\n",
    "SF_XCT = snowflake.connector.connect(**xct_params) #connection object\n",
    "CSR = SF_XCT.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d94af5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4486d4",
   "metadata": {},
   "source": [
    "Step 2: Query the database. \n",
    "\n",
    "Our sampling methodology is an interrupted census (every post for a 5 minute interval, every 3 hours throughout the day), not a random sample, and not a continuous sample.\n",
    "\n",
    "Therefore for timeseries analysis, we GROUP BY the day field to alow for a continuous timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b301f49",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"select\n",
    "  keyword,\n",
    "  topic,\n",
    "  category,\n",
    "  DAY_POST_CREATED_AT,\n",
    "  max(MONTH_POST_CREATED_AT) as month_post_created_at,\n",
    "  max(YEAR_POST_CREATED_AT) as year_post_created_at,\n",
    "  COUNT(author_thread_mentions) daily_author_thread_mentions,\n",
    "  SUM(total_posts_with_keyword) as daily_total_posts_keyword,\n",
    "  SUM(positive_mentions) AS daily_positive_mentions,\n",
    "  SUM(negative_mentions) AS daily_negative_mentions,\n",
    "  SUM(neutral_mentions) AS daily_neutral_mentions,\n",
    "  AVG(pct_positive) as daily_pct_positive,\n",
    " AVG(pct_negative) as daily_pct_negative,\n",
    " AVG(pct_neutral) as daily_pct_neutral,\n",
    "from {SF_DB}.{SF_SC}.timeseries_nlp\n",
    "group by \n",
    "keyword\n",
    ",topic\n",
    ",category\n",
    ",day_post_created_at\n",
    "order by month_post_created_at,day_post_created_at asc;\"\"\" \n",
    "CSR.execute(query)\n",
    "df = CSR.fetch_pandas_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421a01d",
   "metadata": {},
   "source": [
    "Step 3: Format Timestamp Column\n",
    "\n",
    "This step creates a pandas-formatted timestamp that we can use for all the remaining tests in this notebook.\n",
    "\n",
    "Timeseries data columns (hour, day, month, year) are stored as separate numeric columns in the dataset to allow for other groupins and analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "887eea68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           KEYWORD             TOPIC        CATEGORY  DAY_POST_CREATED_AT  \\\n",
      "TIMESTAMP                                                                   \n",
      "2025-07-17   china  global conflicts  issue specific                   17   \n",
      "2025-07-17   trump       US domestic  issue specific                   17   \n",
      "2025-07-17   biden       US domestic  issue specific                   17   \n",
      "2025-07-17   putin  global conflicts  issue specific                   17   \n",
      "2025-07-18   putin  global conflicts  issue specific                   18   \n",
      "\n",
      "            MONTH_POST_CREATED_AT  YEAR_POST_CREATED_AT  \\\n",
      "TIMESTAMP                                                 \n",
      "2025-07-17                      7                  2025   \n",
      "2025-07-17                      7                  2025   \n",
      "2025-07-17                      7                  2025   \n",
      "2025-07-17                      7                  2025   \n",
      "2025-07-18                      7                  2025   \n",
      "\n",
      "            DAILY_AUTHOR_THREAD_MENTIONS DAILY_TOTAL_POSTS_KEYWORD  \\\n",
      "TIMESTAMP                                                            \n",
      "2025-07-17                            17                       167   \n",
      "2025-07-17                            22                      3999   \n",
      "2025-07-17                            16                       266   \n",
      "2025-07-17                            17                       268   \n",
      "2025-07-18                            17                       206   \n",
      "\n",
      "            DAILY_POSITIVE_MENTIONS  DAILY_NEGATIVE_MENTIONS  \\\n",
      "TIMESTAMP                                                      \n",
      "2025-07-17                       24                       69   \n",
      "2025-07-17                      109                     3047   \n",
      "2025-07-17                        8                      209   \n",
      "2025-07-17                        5                      195   \n",
      "2025-07-18                       14                      127   \n",
      "\n",
      "            DAILY_NEUTRAL_MENTIONS DAILY_PCT_POSITIVE DAILY_PCT_NEGATIVE  \\\n",
      "TIMESTAMP                                                                  \n",
      "2025-07-17                      74    13.111120117647    38.244405235294   \n",
      "2025-07-17                     804     6.538820227273    72.664347590909   \n",
      "2025-07-17                      42     3.756127500000    79.651517250000   \n",
      "2025-07-17                      64     1.346630647059    72.586512352941   \n",
      "2025-07-18                      64     5.432661941176    60.127697352941   \n",
      "\n",
      "           DAILY_PCT_NEUTRAL  \n",
      "TIMESTAMP                     \n",
      "2025-07-17   49.379768764706  \n",
      "2025-07-17   20.973297727273  \n",
      "2025-07-17   16.592355375000  \n",
      "2025-07-17   26.525451235294  \n",
      "2025-07-18   34.439640647059  \n"
     ]
    }
   ],
   "source": [
    "df['TIMESTAMP'] = pd.to_datetime(dict(year=df['YEAR_POST_CREATED_AT'], month=df['MONTH_POST_CREATED_AT'], day=df['DAY_POST_CREATED_AT']))\n",
    "\n",
    "df = df.set_index('TIMESTAMP')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75edb0a8",
   "metadata": {},
   "source": [
    "Step 4: Determine which keyword sentiments show a trend vs a stationary. \n",
    "\n",
    "The Augmented Dickey-Fuller Test identifies whether a quantity has a trend within a timeseries.\n",
    "This test cannot determine the direction of the trend on its own. Those keywords showing a trend will be analyzed with decomposition analysis and autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4d4838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adf_stats(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Performs the Augmented Dickey-Fuller test and returns statistics\n",
    "    as dictionary with p-value and stationarity status.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = adfuller(series, autolag='AIC')\n",
    "        p_value = result[1]\n",
    "        is_stationary = p_value <= 0.05\n",
    "        return {'adf_p_value': p_value, 'is_stationary': is_stationary}\n",
    "    except Exception as e:\n",
    "        print(f\"Error during ADF test: {e}\")\n",
    "        return {'adf_p_value': np.nan, 'is_stationary': np.nan}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52507ad",
   "metadata": {},
   "source": [
    "Step 5: Perform timeseries decomposition. If there is a trend, this separates out seasonality and residual.\n",
    "\n",
    "We will also compare the residuals from both additive and multiplicative models. The one with a more random distribution of residuals is the better match to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62dd99af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decomposition_stats(series: pd.Series, model: str, series_name: str):\n",
    "    \"\"\"\n",
    "    Performs seasonal decomposition on a time series using a specified model\n",
    "    and returns statistics as a dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Default is 7 day period for weekly seasonality component.\n",
    "        decomposition = seasonal_decompose(series, model=model, period=7)\n",
    "\n",
    "        # Populate dictionary output with decomp statistics: trend, seasonal, residual\n",
    "        trend_stats = {\n",
    "            'trend_mean': decomposition.trend.mean(),\n",
    "            'trend_std': decomposition.trend.std()\n",
    "        }\n",
    "        \n",
    "        seasonal_stats = {\n",
    "            'seasonal_range': decomposition.seasonal.max() - decomposition.seasonal.min()\n",
    "        }\n",
    "\n",
    "        residual_stats = {\n",
    "            'residual_std': decomposition.resid.std()\n",
    "        }\n",
    "\n",
    "        # Combine all stats into a single dictionary\n",
    "        all_decomp_stats = {**trend_stats, **seasonal_stats, **residual_stats}\n",
    "        \n",
    "        # Add a prefix to each key to indicate the decomposition model used\n",
    "        prefixed_decomp_stats = {f\"{model}_{series_name}_{key}\": value for key, value in all_decomp_stats.items()}\n",
    "        return prefixed_decomp_stats\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during {model} decomposition for {series_name} series: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs the full timeseries analysis pipeline\n",
    "def timeseries_analysis_pipeline(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Runs a statistical analysis pipeline on the timeseries DataFrame ('df' by default here).\n",
    "    It expects a DataFrame with a 'timestamp' column for daily time series index\n",
    "    and the 'KEYWORD' column for grouping the data. Keywords are determined by table object in Snowflake (steets schema) prior to analysis.\n",
    "    It will perform ADF tests\n",
    "    and conditional decomposition on the specified data columns.\n",
    "    \"\"\"\n",
    "    # Define the metric columns for analysis-- both percentages and thread counts.\n",
    "    time_series_columns = [\n",
    "        'DAILY_PERCENT_POSITIVE',\n",
    "        'DAILY_PERCENT_NEGATIVE',\n",
    "        'DAILY_POSITIVE_MENTIONS',\n",
    "        'DAILY_NEGATIVE_MENTIONS'\n",
    "    ]\n",
    "    \n",
    "    # Define the two model typs we are running\n",
    "    decomposition_models = ['additive', 'multiplicative']\n",
    "    \n",
    "    # Group the DataFrame by keyword for a row-by-row analysis\n",
    "    grouped = df.groupby('KEYWORD')\n",
    "    \n",
    "    #store results\n",
    "    pipeline_results = []\n",
    "    \n",
    "    for KEYWORD, group_df in grouped:\n",
    "        # Sort by TIMESTAMP to ensure the time series is in order\n",
    "        group_df = group_df.set_index('TIMESTAMP').sort_index()\n",
    "        \n",
    "        # Create a base dictionary for this keyword's results\n",
    "        row_stats = {'KEYWORD': KEYWORD}\n",
    "        \n",
    "        # Loop through each specified time series column\n",
    "        for col in time_series_columns:\n",
    "            series = group_df[col]\n",
    "            \n",
    "            # First Pass: Run the ADF test\n",
    "            adf_stats = get_adf_stats(series)\n",
    "            \n",
    "            # Add ADF results to the dictionary with a column-specific prefix\n",
    "            for key, value in adf_stats.items():\n",
    "                row_stats[f\"{col}_{key}\"] = value\n",
    "\n",
    "            # Second Pass: Conditionally run the decomposition tests\n",
    "            if not adf_stats['is_stationary']:\n",
    "                print(f\"Running decomposition for '{KEYWORD}' on column '{col}'...\")\n",
    "                for model in decomposition_models:\n",
    "                    stats = get_decomposition_stats(series, model, col)\n",
    "                    row_stats.update(stats)\n",
    "            else:\n",
    "                print(f\"Skipping decomposition for '{KEYWORD}' on column '{col}' (stationary).\")\n",
    "                # Create placeholder columns with NA values\n",
    "                for model in decomposition_models:\n",
    "                    for stat_key in ['trend_mean', 'trend_std', 'seasonal_range', 'residual_std']:\n",
    "                        row_stats[f\"{model}_{col}_{stat_key}\"] = np.nan\n",
    "        \n",
    "        pipeline_results.append(row_stats)\n",
    "    \n",
    "    # Convert the list of dictionaries to the final summary DataFrame\n",
    "    final_df = pd.DataFrame(pipeline_results)\n",
    "    \n",
    "    print(\"--- Final Summary Table with Conditional Test Results ---\")\n",
    "    print(final_df.to_string())\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09828967",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "timeseries_analysis_pipeline() got an unexpected keyword argument 'keyword_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m timeseries_analysis_pipeline(df, keyword_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKEYWORD\u001b[39m\u001b[38;5;124m'\u001b[39m, timestamp_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTIMESTAMP\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: timeseries_analysis_pipeline() got an unexpected keyword argument 'keyword_column'"
     ]
    }
   ],
   "source": [
    "timeseries_analysis_pipeline(df, keyword_column='KEYWORD', timestamp_column='TIMESTAMP')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
